{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q qdrant-client\n",
    "%pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fastembed no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out qdrant for future RAG set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "# Initialise Qdrant client\n",
    "client = QdrantClient(\":memory:\")  # Use in-memory for demo; replace with actual endpoint for production\n",
    "\n",
    "# Check if the collection already exists\n",
    "collection_name = \"demo_collection\"\n",
    "if not client.collection_exists(collection_name):\n",
    "    # Create the collection if it doesn't exist\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=3, distance=Distance.EUCLID)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "# Example vectors\n",
    "vectors = [\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "]\n",
    "\n",
    "# Insert vectors with unique IDs\n",
    "for idx, vector in enumerate(vectors):\n",
    "    client.upsert(\n",
    "        collection_name=\"demo_collection\",\n",
    "        points=[\n",
    "            PointStruct(id=idx, vector=vector)  # Correct structure\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = [1.1, 2.1, 3.1]\n",
    "\n",
    "# Perform the search\n",
    "search_results = client.search(\n",
    "    collection_name=\"demo_collection\",\n",
    "    query_vector=query_vector,\n",
    "    limit=2  # Number of closest matches to return\n",
    ")\n",
    "\n",
    "# Display search results\n",
    "for result in search_results:\n",
    "    print(f\"ID: {result.id}, Distance: {result.score}, Vector: {result.vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"my_embeddings_collection\"\n",
    "\n",
    "# Check if the collection already exists\n",
    "if client.collection_exists(collection_name):\n",
    "    # Optionally drop the existing collection if you want to recreate it\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "# Create the collection\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\"size\": 768, \"distance\": \"Cosine\"}  # Adjust `size` to match your embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"general_advice.txt\", \"r\") as file:\n",
    "    text_data = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize  # Example using NLTK\n",
    "\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        if len(\" \".join(current_chunk).split()) > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "text_chunks = chunk_text(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import FastEmbed \n",
    "\n",
    "model = FastEmbed(model_name=\"test_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    embedding = model.embed_text(chunk)\n",
    "    embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed_texts(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = np.array(embeddings)  # Convert list of embeddings to a NumPy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity Test: Similar texts should have a high cosine similarity (close to 1), while dissimilar texts should have a low cosine similarity (close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "similarity = cosine_similarity(embeddings[0], embeddings[1])\n",
    "print(f\"Cosine Similarity between first two chunks: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbor Search: To further test the embeddings, nearest neighbor search within the embedding space will allow to see if similar text chunks are clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn_model = NearestNeighbors(n_neighbors=2, metric='cosine')\n",
    "nn_model.fit(embeddings)\n",
    "\n",
    "# Find the nearest neighbor for the first chunk\n",
    "distances, indices = nn_model.kneighbors([embeddings[0]])\n",
    "\n",
    "# Output the closest neighbor\n",
    "print(f\"Closest neighbor to the first chunk is chunk at index: {indices[0][1]} with distance: {distances[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality for visualization (e.g., from 768D to 2D)\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot the reduced embeddings\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])\n",
    "\n",
    "# Optionally, label the points with chunk indices or keywords\n",
    "for i, text_chunk in enumerate(text_chunks):\n",
    "    plt.annotate(str(i), (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "\n",
    "plt.title('PCA of Text Chunk Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1 = text_chunks[0]\n",
    "chunk2 = text_chunks[1]\n",
    "similarity = cosine_similarity(embeddings[0], embeddings[1])\n",
    "\n",
    "print(f\"Chunk 1: {chunk1}\\n\")\n",
    "print(f\"Chunk 2: {chunk2}\\n\")\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
