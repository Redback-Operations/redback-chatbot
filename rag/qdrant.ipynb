{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q qdrant-client\n",
    "%pip install -q nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q fastembed no-deps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing out qdrant for future RAG set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance\n",
    "\n",
    "# Initialise Qdrant client\n",
    "client = QdrantClient(\":memory:\")  # Use in-memory for demo; replace with actual endpoint for production\n",
    "\n",
    "# Check if the collection already exists\n",
    "collection_name = \"demo_collection\"\n",
    "if not client.collection_exists(collection_name):\n",
    "    # Create the collection if it doesn't exist\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=VectorParams(size=3, distance=Distance.EUCLID)\n",
    "    )\n",
    "else:\n",
    "    print(f\"Collection '{collection_name}' already exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qdrant_client.http.models import PointStruct\n",
    "\n",
    "# Example vectors\n",
    "vectors = [\n",
    "    [1.0, 2.0, 3.0],\n",
    "    [4.0, 5.0, 6.0],\n",
    "    [7.0, 8.0, 9.0],\n",
    "]\n",
    "\n",
    "# Insert vectors with unique IDs\n",
    "for idx, vector in enumerate(vectors):\n",
    "    client.upsert(\n",
    "        collection_name=\"demo_collection\",\n",
    "        points=[\n",
    "            PointStruct(id=idx, vector=vector)  # Correct structure\n",
    "        ]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = [1.1, 2.1, 3.1]\n",
    "\n",
    "# Perform the search\n",
    "search_results = client.search(\n",
    "    collection_name=\"demo_collection\",\n",
    "    query_vector=query_vector,\n",
    "    limit=2  # Number of closest matches to return\n",
    ")\n",
    "\n",
    "# Display search results\n",
    "for result in search_results:\n",
    "    print(f\"ID: {result.id}, Distance: {result.score}, Vector: {result.vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collection_name = \"my_embeddings_collection\"\n",
    "\n",
    "# Check if the collection already exists\n",
    "if client.collection_exists(collection_name):\n",
    "    # Optionally drop the existing collection if you want to recreate it\n",
    "    client.delete_collection(collection_name)\n",
    "\n",
    "# Create the collection\n",
    "client.create_collection(\n",
    "    collection_name=collection_name,\n",
    "    vectors_config={\"size\": 768, \"distance\": \"Cosine\"}  # Adjust `size` to match your embedding dimension\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"general_advice.txt\", \"r\") as file:\n",
    "    text_data = file.read()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize  # Example using NLTK\n",
    "\n",
    "def chunk_text(text, max_tokens=512):\n",
    "    sentences = sent_tokenize(text)\n",
    "    chunks = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        current_chunk.append(sentence)\n",
    "        if len(\" \".join(current_chunk).split()) > max_tokens:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "            current_chunk = []\n",
    "\n",
    "    if current_chunk:\n",
    "        chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "    return chunks\n",
    "\n",
    "text_chunks = chunk_text(text_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastembed import FastEmbed \n",
    "\n",
    "model = FastEmbed(model_name=\"test_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = []\n",
    "\n",
    "for chunk in text_chunks:\n",
    "    embedding = model.embed_text(chunk)\n",
    "    embeddings.append(embedding)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.embed_texts(text_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "embeddings = np.array(embeddings)  # Convert list of embeddings to a NumPy array\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cosine Similarity Test: Similar texts should have a high cosine similarity (close to 1), while dissimilar texts should have a low cosine similarity (close to 0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return 1 - cosine(embedding1, embedding2)\n",
    "\n",
    "similarity = cosine_similarity(embeddings[0], embeddings[1])\n",
    "print(f\"Cosine Similarity between first two chunks: {similarity}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nearest Neighbor Search: To further test the embeddings, nearest neighbor search within the embedding space will allow to see if similar text chunks are clustered together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "nn_model = NearestNeighbors(n_neighbors=2, metric='cosine')\n",
    "nn_model.fit(embeddings)\n",
    "\n",
    "# Find the nearest neighbor for the first chunk\n",
    "distances, indices = nn_model.kneighbors([embeddings[0]])\n",
    "\n",
    "# Output the closest neighbor\n",
    "print(f\"Closest neighbor to the first chunk is chunk at index: {indices[0][1]} with distance: {distances[0][1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Reduce dimensionality for visualisation (e.g., from 768D to 2D)\n",
    "pca = PCA(n_components=2)\n",
    "reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "# Plot the reduced embeddings\n",
    "plt.scatter(reduced_embeddings[:, 0], reduced_embeddings[:, 1])\n",
    "\n",
    "# Optionally, label the points with chunk indices or keywords\n",
    "for i, text_chunk in enumerate(text_chunks):\n",
    "    plt.annotate(str(i), (reduced_embeddings[i, 0], reduced_embeddings[i, 1]))\n",
    "\n",
    "plt.title('PCA of Text Chunk Embeddings')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk1 = text_chunks[0]\n",
    "chunk2 = text_chunks[1]\n",
    "similarity = cosine_similarity(embeddings[0], embeddings[1])\n",
    "\n",
    "print(f\"Chunk 1: {chunk1}\\n\")\n",
    "print(f\"Chunk 2: {chunk2}\\n\")\n",
    "print(f\"Cosine Similarity: {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qdrant Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not client.collection_exists(collection_name):\n",
    "    client.create_collection(\n",
    "        collection_name=collection_name,\n",
    "        vectors_config=vectors_config\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.update_collection(\n",
    "    collection_name=collection_name,\n",
    "    config={\n",
    "        \"hnsw_config\": {\n",
    "            \"ef_construct\": 200,  # Construction time/space trade-off\n",
    "            \"m\": 16  # Higher value gives more accurate results but uses more memory\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve collection info to verify setup\n",
    "collection_info = client.get_collection(collection_name)\n",
    "print(f\"Collection Info: {collection_info}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example vector for testing\n",
    "test_vector = np.random.rand(768).tolist()  # Random 768-dimensional vector\n",
    "\n",
    "# Insert the vector into the collection\n",
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=[\n",
    "        {\n",
    "            \"id\": 1,  # Unique ID for this vector\n",
    "            \"vector\": test_vector\n",
    "        }\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Retrieve the vector\n",
    "retrieved_vector = client.retrieve(\n",
    "    collection_name=collection_name,\n",
    "    ids=[1]\n",
    ")\n",
    "print(f\"Retrieved Vector: {retrieved_vector}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the Generated Vector Embeddings into the Qdrant Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "points = [\n",
    "    {\n",
    "        \"id\": idx,  # Unique ID for each vector\n",
    "        \"vector\": embedding,  # The vector embedding\n",
    "        \"payload\": {\"text\": text_chunk}  # metadata\n",
    "    }\n",
    "    for idx, (embedding, text_chunk) in enumerate(zip(embeddings, text_chunks))\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client.upsert(\n",
    "    collection_name=collection_name,\n",
    "    points=points\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve vectors by ID to verify\n",
    "retrieved_vectors = client.retrieve(\n",
    "    collection_name=collection_name,\n",
    "    ids=[0, 1, 2]  # Retrieve the first three vectors\n",
    ")\n",
    "print(f\"Retrieved Vectors: {retrieved_vectors}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the Retrieval Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vector = embeddings[0]\n",
    "\n",
    "# Perform a search in the collection\n",
    "search_results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=3  # Number of closest neighbors to return\n",
    ")\n",
    "\n",
    "# Output the search results\n",
    "print(\"Search Results:\")\n",
    "for result in search_results:\n",
    "    print(f\"ID: {result['id']}, Score: {result['score']}, Text: {result['payload']['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform the search\n",
    "search_results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=3\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "retrieval_time = end_time - start_time\n",
    "\n",
    "print(f\"Retrieval Time: {retrieval_time:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the retrieved text with the query text\n",
    "query_text = text_chunks[0]\n",
    "print(f\"Query Text: {query_text}\\n\")\n",
    "\n",
    "for result in search_results:\n",
    "    retrieved_text = result['payload']['text']\n",
    "    print(f\"Retrieved Text: {retrieved_text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with multiple vectors\n",
    "for i in range(5):  # Test with the first 5 vectors\n",
    "    query_vector = embeddings[i]\n",
    "    search_results = client.search(\n",
    "        collection_name=collection_name,\n",
    "        query_vector=query_vector,\n",
    "        limit=3\n",
    "    )\n",
    "    print(f\"\\nSearch results for query {i}:\")\n",
    "    for result in search_results:\n",
    "        print(f\"ID: {result['id']}, Score: {result['score']}, Text: {result['payload']['text']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimise search by adjusting the 'ef' parameter\n",
    "search_results = client.search(\n",
    "    collection_name=collection_name,\n",
    "    query_vector=query_vector,\n",
    "    limit=3,\n",
    "    params={\"hnsw_ef\": 200}  # Higher 'ef' values typically increase accuracy\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
